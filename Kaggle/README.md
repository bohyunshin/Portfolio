## Kaggle 프로젝트



### 프로젝트에 대해

학회에서 처음으로 수행한 프로젝트입니다. 초심자답게 Kaggle에 도전하였습니다. 관련 링크는 [여기](https://www.kaggle.com/c/DontGetKicked) 에서 확인하실 수 있습니다.



### 최종 순위

570팀 중 7등



### 프로젝트를 하게된 동기

데이터 사이언스를 처음으로 접한 시기였기 때문에, 데이터 분석을 처음부터 끝까지 경험해보는 것이 중요하다고 생각했습니다. 따라서 팀원을 모아서, Kaggle 프로젝트를 시작하게 되었습니다.



### 사용 기술

* Python



### 공모전에서 맡은 부분

* Imbalance Classification 해결책에 대한 고민



### 얻은 것

* 불균형 데이터에 대해

  불균형 데이터라면 일반적인 ML을 사용하는 것 외에 SMOTE, NEARMISS 등의 샘플링 기법이나 sample_weight과 관련된 파라미터 변수를 조정해줘야할 때가 많습니다. 이에 대해서 어떻게 대응해야할지 팀원들과 해결책을 논의하였습니다.

* 범주형 변수가 매우 많은 상황에 대해

  범주형 변수를 모형에 그대로 넣을 수 없고 모형이 이해할 수 있도록 dummay variable 등의 인코딩 작업이 필요합니다. 그런데, tree 기반 모형에서 매우 많은 dummay variable을 집어넣는다면 tree를 제대로 키우지 못할 가능성이 큽니다. 따라서 저희 팀은 LightGBM, CatBoost, H2O RF 등을 시도하였고, 결과가 가장 좋은 CatBoost을 택하였습니다.

* CatBoost

  이 부스팅 기법은 다른 부스팅 기법과 다르게 범주형 변수를 그대로 모형에 집어 넣어도 됩니다. XGBoost는 예측력은 좋지만 범주형 변수를 모두 변환해야 하는 단점이 있습니다. 사실, real data를 분석할 때 범주형 변수의 비중이 높은 경우가 많았는데, 이번에 CatBoost을 사용한 경험을 기반으로 추후에도 이런 상황에 대해서 CatBoost을 유연하게 사용할 수 있었습니다.

