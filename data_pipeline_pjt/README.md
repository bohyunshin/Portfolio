## 데이터 파이프라인 프로젝트



### 프로젝트에 대해

END-TO-END 데이터 파이프라인 프로젝트입니다. 데이터 수집, 저장, 시각화 모두를 구현해보았습니다.



### 프로젝트를 하게된 동기

데이터를 동적으로 보기 시작하며, 동적으로 유입되는 데이터를 자동으로 처리하는 것에 관심이 많아졌습니다. 이전에 카프카 미니 프로젝트를 통해서 간단하게 카프카 producer / consumer을 구현하였는데 의미있는 데이터로 현업에서 많이 사용하는 elastic을 사용하여 프로젝트를 진행하고 싶었습니다.



### 사용 기술

* AWS
* filebeat
* kafka
* nifi
* elasticsearch
* kibana



### 프로젝트에서 맡은 부분

처음부터 끝까지 전 부분을 혼자 진행하였습니다.



### 얻은 것

* 데이터 파이프라인에 대한 이해

  프로젝트를 처음부터 끝까지 진행하며 데이터 흐름에 대한 이해를 할 수 있었습니다. 어떤 식으로 데이터를 가져오고 이 데이터를 적절한 플랫폼에 어떻게 둘 것이며 최종 시각화는 어떻게 할 것인지를 고민하며 동적으로 유입되는 데이터를 처리하는 기본적인 방법을 접할 수 있었습니다.

* 에러를 두려워하지 말자

  에러는 필연적으로 발생할 수밖에 없습니다. 예전에는 에러가 발생하기만하면 한숨부터 나왔는데, 이번 플젝을 하며 여러 에러를 해결하다보니 이 에러를 빨리 해결해서 성취감을 얻고자 하는 마음이 더 커졌습니다. 특히 에러는 그냥 발생하지 않고 로그 데이터를 면밀하게 보면 답이 분명히 있음을 확실하게 느꼈습니다. 대부분의 제가 직면한 에러는 이미 많은 사람들이 직면했던 에러이며 로그 데이터에서 에러가 발생하는 부분을 확실하게 잡아내고 그 부분을 검색한다면 솔루션은 이미 제시되어 있었습니다. 이와 같이, 저는 이번 플젝을 통해서 로그 데이터를 면밀히 분석하고, 거기에서 에러 메세지를 추출하자는 교훈을 얻었습니다.



### 자세한 프로젝트 내용

[여기](https://github.com/bohyunshin/Portfolio/blob/master/data_pipeline_pjt/final_ppt.pdf)를 참고해주시기 바랍니다.

